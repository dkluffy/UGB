# -*- coding: utf-8 -*-
"""ugb_test.v2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-pI0JIHojPsQLC_uCQj0G-emePnopVt
"""

# Commented out IPython magic to ensure Python compatibility.
# %%

# %tensorflow_version 2.x

import os

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
from datetime import datetime
import time

from dataloader import fetch_image_label,create_dataset,create_dataset_in_mem
from tools import write_images_ds
from utils import pprint
import baseconf as bcf

def downsample(filters, size, apply_batchnorm=True,apply_pool=True):
    initializer = tf.random_normal_initializer(0., 0.02)
  
    result = tf.keras.Sequential()
    result.add(
        layers.Conv2D(filters, size, strides=2, padding='same',
                               kernel_initializer=initializer, use_bias=False))
  
    if apply_batchnorm:
      result.add(layers.BatchNormalization())
  
    result.add(layers.LeakyReLU())

    if apply_pool:
      result.add(layers.MaxPooling2D())
    
  
    return result

def make_discriminator_model():
    
    inp = layers.Input(shape=[128, 128, 3], name='input_image')
    tar = layers.Input(shape=[128, 128, 3], name='target_image')
  
    x = layers.concatenate([inp, tar]) # (bs, 128, 128, channels*2)
  
    x = downsample(64, 4, False)(x) 
    x = downsample(128, 4)(x) 
    x = downsample(256, 4)(x)
    x = downsample(512, 1,True,False)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(512,activation='selu')(x)
    x = layers.BatchNormalization()(x)
    last = layers.Dense(1,activation='sigmoid')(x)
  
    return tf.keras.Model(inputs=[inp, tar], outputs=last)

###################HParams##############################
BATCH_SIZE=8
noise_dim=512
base_learning_rate=1e-4
save_interval=5
# Optimizers
# generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
# discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

generator_optimizer = tf.keras.optimizers.Adam()
discriminator_optimizer = tf.keras.optimizers.Adam()
# lr callback
def lr_scheduler(epoch):
  learning_rate = base_learning_rate
  if epoch > 10:
    learning_rate =base_learning_rate/2
  if epoch > 20:
    learning_rate = base_learning_rate/10
  if epoch > 30:
    learning_rate = base_learning_rate * tf.math.exp(0.1 * (10 - epoch))
  #for tensorboard
  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)
  return learning_rate

LAMBDA = 10

#tb callback
run_logdir = "/content/drive/My Drive/ugb"


##############################################################################

discriminator = make_discriminator_model()
discriminator.summary()

# 该方法返回计算交叉熵损失的辅助函数
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(disc_real_output, disc_generated_output):
  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)
  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)
  total_disc_loss = real_loss + generated_loss

  return total_disc_loss,real_loss

# checkpoint and fitlog
checkpoint = tf.train.Checkpoint(discriminator=discriminator)
ckpt_manager = tf.train.CheckpointManager(checkpoint, run_logdir+"/v2", max_to_keep=3)

fit_log = os.path.join(run_logdir,"fit_v2",datetime.now().strftime("%Y%m%d-%H%M%S"))
summary_writer = tf.summary.create_file_writer(fit_log)

# single train step
@tf.function
def train_step(target,noise, epoch):
  #noise = tf.random.normal([BATCH_SIZE, noise_dim])
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    #gen_output = generator(noise, training=True)

    disc_real_output = discriminator([target, target], training=True)
    disc_generated_output = discriminator([target, noise], training=True)

    #gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
    disc_loss,real_loss = discriminator_loss(disc_real_output, disc_generated_output)

  #generator_gradients = gen_tape.gradient(gen_total_loss,
                                          #generator.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               discriminator.trainable_variables)
  
  #generator_optimizer.apply_gradients(zip(generator_gradients,
                                          #generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              discriminator.trainable_variables))

  with summary_writer.as_default():
    #tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)
    #tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)
    tf.summary.scalar('real_loss', real_loss, step=epoch)
    tf.summary.scalar('disc_loss', disc_loss, step=epoch)
  return disc_loss,real_loss
# def generate_and_save_images(model, epoch, test_input):
#     # 注意 training` 设定为 False
#     # 因此，所有层都在推理模式下运行（batchnorm）。
#     #predictions = model(test_input, training=False)
#     #write_images_tb(predictions)

mse = tf.keras.losses.MeanSquaredError()
def eveluator(targets_img,noise_img):
  """
  这里只测试，目标图片（随机变换）和真实随机噪声（即不包含目标的图片）
  """
  result = discriminator([targets_img,noise_img], training=False)
  ev_loss = mse(tf.zeros_like(result),result)
  return ev_loss

def train(new_train,X_train,X_val,epochs,init_epoch=1,steps_per_epoch=100,val_steps=10):
  """
  X_train - ds: 只包含目标图片
  X_val - ds: 只包含noise
  """
  checkpoint.restore(ckpt_manager.latest_checkpoint)
  if ckpt_manager.latest_checkpoint:
    print("Restored from {}".format(ckpt_manager.latest_checkpoint))
  else:
    print("Initializing from scratch.")
  
  print("Fire to Train....")
  for epoch in range(init_epoch,epochs):
    
    start = time.time()

    generator_optimizer = tf.keras.optimizers.Adam(lr_scheduler(epoch))
    discriminator_optimizer = tf.keras.optimizers.Adam(lr_scheduler(epoch))

    for step in range(1,steps_per_epoch):
      for targets,noise in new_train.take(1):
        disc_loss,real_loss = train_step(targets,noise,epoch)
        if step % 25 == 0:
          print( "EPOCH-[%s/%s]- real_loss:%.4f , DISC LOSS:%.4f (%s/%s) " % \
                    (epoch,epochs,real_loss,disc_loss,step,steps_per_epoch))

    
    # 保存一次模型
    if epoch % save_interval == 0:
      save_path = ckpt_manager.save()
      print("Saved checkpoint for epoch {}: {}".format(epoch, save_path))
      # chkfilename = "gan_checkpoints" \
      #       +"."+datetime.now().strftime("%Y%m%d_%H%M") \
      #       +( ".epoch-%s.ckpt" % epoch)
      # checkpoint_prefix = os.path.join(run_logdir, chkfilename)
      # checkpoint.save(file_prefix = checkpoint_prefix)

    # epoch end
    print ('Time for epoch {} is {:.2f} sec'.format(epoch + 1, time.time()-start))


# %%

from dataloader import load_and_preprocess_image,load_image,im_rescale,resize_with_pad,preprocess_image
def create_ds(images,lables,im_loader=load_and_preprocess_image,buffer_size=128):
    path_ds = tf.data.Dataset.from_tensor_slices(images)
    image_ds = path_ds.map(im_loader)
    return image_ds.shuffle(buffer_size=buffer_size).repeat()

def create_ds_in_mem(images,lables,buffer_size=128):
    ratio = bcf.RESCAL_RATIO
    m_size=bcf.MID_SIZE
    
    copys_len = len(ratio)+1
    
    img_list = []
    for im in images:
        im = load_image(im)
        tmp_list=im_rescale(im,ratio=ratio,t_size=m_size)
        tmp_list.append(resize_with_pad(im,m_size,m_size))
        img_list+=tmp_list

    new_lables = []
    for lb in lables:
        new_lables+=[lb]*copys_len

    #assert len(img_list) == len(new_lables),"img_list,new_lables is not match!"

    image_ds = tf.data.Dataset.from_tensor_slices(img_list)
    label_ds = tf.data.Dataset.from_tensor_slices(new_lables)

    image_ds = image_ds.map(preprocess_image)
    
    return image_ds.shuffle(buffer_size=buffer_size).repeat()

val_dir = run_logdir+"/noise"
targets_dir = run_logdir+"/targets"
tg_imgs,tg_labels = fetch_image_label(targets_dir)
val_imgs,val_labels = fetch_image_label(val_dir,0)
X_train = create_ds_in_mem(tg_imgs,tg_labels).batch(BATCH_SIZE)
X_val = create_ds(val_imgs,val_labels).batch(BATCH_SIZE)

new_train = tf.data.Dataset.zip((X_train,X_val))

for x,y in new_train.take(1):
  print(x.shape)

train(new_train,X_train,X_val,100,1)

cd /content/drive/My\ Drive/ugb

!ls

!tar czf chk.colab.150epoch.tar.gz ckpt-33.* checkpoint

!tar czf fit_log.150epoch.tar.gz fit/

!ls -alh

"""# eveluat"""

